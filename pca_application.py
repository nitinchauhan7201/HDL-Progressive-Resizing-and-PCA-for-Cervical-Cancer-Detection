# -*- coding: utf-8 -*-
"""pca_application.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGESBLNKpQwmhWXphLylLd07y_Ay7qya
"""

"""LOAD DATA"""
# path to folder with data
import os
os.getcwd()
os.chdir(r"C:\Users\nitin\Desktop")
file_path = "Cervical_Cancer_Detection"

import numpy as np
import h5py
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import os

def load_weights(file_path):
    """Load weights from .h5 file."""
    with h5py.File(file_path, 'r') as f:
        # Adjust based on how weights are stored in your file
        weights = f['weights'][:]  # Make sure 'weights' is the correct key
    return weights

def apply_pca_and_save(weights, pca_file_path):
    """Apply PCA and save reduced features."""
    # Standardize the features
    scaler = StandardScaler()
    weights_scaled = scaler.fit_transform(weights)

    # Apply PCA to retain 95% variance
    pca = PCA(n_components=0.95)
    weights_pca = pca.fit_transform(weights_scaled)

    # Save PCA-reduced weights
    np.save(pca_file_path, weights_pca)

if __name__ == "__main__":
    weight_files = [
        'concatenate_model1.h5',
        'concatenate_model2.h5',
        'concatenate_model3.h5'
    ]
    pca_file_paths = [
        'pca_weights_reduced1.npy',
        'pca_weights_reduced2.npy',
        'pca_weights_reduced3.npy'
    ]

    for weight_file, pca_file_path in zip(weight_files, pca_file_paths):
        print(f"Processing {weight_file}...")
        weights = load_weights(weight_file)
        apply_pca_and_save(weights, pca_file_path)
        print(f"PCA applied and saved to {pca_file_path}")